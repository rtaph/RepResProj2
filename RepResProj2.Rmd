---
title: 'Reproducible Research: Peer Assessment 2'
output:
  html_document:
    keep_md: yes
  pdf_document: default
---
******************************
## Most Harmful Weather Events

The purpose of the analysis is to determine which types of events are most harmful with respect to population health in the United States by using the [NOAA Storm Database](http://www.ncdc.noaa.gov/stormevents/). The paper attempts to answer two basic questions about severe weather events:

1. Across the United States, which types of events are most harmful with respect to population health?
2. Across the United States, which types of events have the greatest economic consequences?

### Synopsis

My analysis reveals that tornadoes and excessive heat are the most harmful weather event to human health in the United States. These conclusions are based on the total fatalities as well as injuries recorded for each type of event. Floods are the most economically damaging weather event, costing the economy approximately USD 144 billion per year.


### Data Processing
I begin the analysis by loading libraries and setting a few global parameters:

```{r chunkOpts, cache=TRUE}
  ## load needed libraries, set global options, and working directory
  library(knitr); library(plyr)
  opts_chunk$set(echo=TRUE)       
  setwd("~/Documents/Courses/datasciencecoursera/RepResProj2/")
```

I then check that the data file exists, download it (if needed), and unzip it:
```{r chuckDownload, cache=TRUE}
  #Download file if it does not exist
  if (!file.exists("repdata-data-StormData.csv.bz2")) {
      fileURL <- "http://bit.ly/1uNSAQY"
      zipfile = "repdata-data-StormData.csv.bz2"
      download.file(fileURL, destfile=zipfile, method="curl")
  }
```

The data is then read into R. As it is a large file, data is first read as character strings to improve performance and speed: 

```{r chunkLoadData, cache=TRUE}
  # Load the data and assign it to a variable
  file   = "repdata-data-StormData.csv.bz2"
  raw    =  read.csv(file, stringsAsFactors = FALSE) # FALSE to optimize read speed
```

Information about the data can be summarized using the `str` command:

```{r chunkExpl1, cache=TRUE}
str(raw)
```

Looking at the summary above, variables can be identified that match to the question of interest. In this analysis, I will use `EVTYPE` (the event type), `FATALITIES`, `INJURIES`, `PROPDMG` (monetary estimate of property damage), and `PROPDMGEXP` (unit used for the damage estimate).

Many of these variables need to be converted or manipulated into more workable formats:

```{r chunkReformat, cache=TRUE}
  # reformat data type of key variables
  raw$EVTYPE = as.factor(raw$EVTYPE)
  raw$BGN_DATE = as.POSIXlt(strptime(raw$BGN_DATE,format="%m/%d/%Y %H:%M:%S"))
  raw$DMG = ""
  raw$DMG = mapvalues(raw$PROPDMGEXP, c("B","M","m","K","H","h","0"),
                                      c(1e9,1e6,1e6,1e3,1e2,1e2,1))
  raw$DMG = as.numeric(raw$DMG) * raw$PROPDMG
  raw$DMG = as.numeric(raw$DMG)
```

A new variable `DMG` is created to capture the monetary estimate of damages from weather events in a universal unit of measure. Although there are certain uncaught response types that cause NAs to be coerced, these cases are ambigous to interpret (even after reading the data help files). As they are few enough, they can likely be ignored without making a big difference on the exploratory analysis. 

By plotting the number of *unique* types of weather events per year below, it is apparent see that the initial period of data (~1950 to 1995) has few categorizations. I find it more likely that this absence of data is a result of lack of collection systems/standards, rather than an absence of particular types of events. I deem it more probable than not that including this initial period would bias the analysis away from type of events that only started being tracked recently.

```{r chunkExpl6, cache=TRUE}
  t = table(format(raw$BGN_DATE,"%Y"))
  u  = as.numeric(tapply(raw$EVTYPE,raw$BGN_DATE[[6]], function(x) length(unique(x))))

  plot(t, type = "l", main = "Number of Records", ylab = "", col = "blue")
  par(new=T); plot(u, type='l', col = "red", lwd = 2, axes=F, xlab=NA, ylab=NA)
  axis(4, col = "red", lwd = 2)
  legend("topleft", c("Total","Unique"), lwd=c(2.5,2.5), col=c("blue","red"))
```

The number of unique weather events jumps sharply in `r names(t[which.max(u)])` (up to `r max(u)` records). Accordingly, I will work only with the subset of data from this date forward, as it is more representative, and will not bias the data towards type of weather events that were tracked earlier on in history.

```{r chunkSubset, cache=TRUE, autodep = TRUE}
  df = raw[raw$BGN_DATE >= "1995-01-01 00:00:00",]
```
This subset nonetheless captures `r round(nrow(df)/nrow(raw)*100,1)`% of the observations in the raw data.

### Results

To answer the research questions, I will analyse which weather events cause the greatest number of fatalities, injuries, and economic damage.

To facilitate this, I have written a re-usable function that generate a summary of the absolute and relative impact of a particular parameter, by event type:

```{r chunkFunc, cache=TRUE}
  # a function that makes a summary table of a given parameter and calculates
  # relative impact (per weather event)
  mktab = function(variable){
    v = df[[variable]]
    dfa = aggregate(v ~ EVTYPE, data = df, sum)
    dfb = aggregate(v ~ EVTYPE, data = df, length)
    names(dfb)[2] = "OCCURENCES"; names(dfa)[2] = variable
    out = merge(dfa, dfb)
    out$RELIMPACT = out[[variable]] / out$OCCURENCES
    out = out[order(out[[variable]], decreasing = T),]
    out
  }
```

I use this function to explore the most harmful weather events, both on the basis of total fatalities and total injuries:

```{r chunkExpl2, cache=TRUE, autodep = TRUE}
  df1 = mktab("FATALITIES"); head(df1)
```

```{r chunkExpl3, cache=TRUE, autodep = TRUE}
  df2 = mktab("INJURIES"); head(df2)
```

With respect to the second research question, I will use the `DMG` parameter. These property damage estimates were computed in the data processing stage using the `PROPDMG` and `PROPDMGEXP` variables. Using the same analysis as before, we obtain a table of the most costly events:

```{r chunkExpl7, cache=TRUE, autodep = TRUE}
  df3 = mktab("DMG"); head(df3)
  barplot(head(df3$DMG), main = "Weather Events Causing the Greatest Economic Damage, 1995-2011", ylab = "Estimated Cost (USD)")
  axis(1, at = 1:6, labels = head(df3$EVTYPE))
```

The above chart shows that on an absolute basis, floods have been the most costly weather event to Americans (USD `r round(max(df3$DMG)/1e9, 1)` billion). This is followed by hurricanes/typhoons and storm surges.

However, by looking at the relative impact, we see that heaviest costs per event comes from `HEAVY RAIN/SEVERE WEATHER` (USD `r round(max(df3$RELIMPACT)/1e9, 1)` billion).

```{r chunkExpl9, cache=TRUE, autodep = TRUE}
  df3[which.max(df3$RELIMPACT),]
```

With only one observation above, this result appears suspect/mistaken. However, a review of the `REMARKS` variable for this entry reveals that this weather event was indeed substantial and costly:

```{r chunkExpl10, cache=TRUE, autodep = TRUE}
  df[df$EVTYPE %in% "HEAVY RAIN/SEVERE WEATHER" & df$PROPDMG == "2.5", "REMARKS"]
```

### Session Info

```{r chunkSessionInfo}
  sessionInfo()
```

